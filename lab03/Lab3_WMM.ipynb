{"cells":[{"cell_type":"markdown","metadata":{"id":"89OsaIyW20yd"},"source":["![An image](https://www.dl.dropboxusercontent.com/s/opt3vuyubou8dgl/demoSlide1.png)\n","---\n","#**Wstęp**\n","`ver. 03.2022`\n","\n","---\n","Notatnik jest podzielony na dwie sekcje. W pierwszej sekcji pt. \"Synteza mowy - TTS\" zaimplementowanych jest kilka otwarto-źródłowych algorytmów syntezy mowy TTS (Text-To-Speech). W drugiej sekcji znajduje się implementacja wybranych algorytmów rozpoznawania mowy z tekstu pisanego STT (Speech-To-Text).\n","\n","<mark>Szczególnie zalecane jest wykorzystanie przeglądarki Google Chrome</mark>\n"]},{"cell_type":"markdown","metadata":{"id":"mhFcR0NF9ZUs"},"source":["---\n","\n","\n","# **Polecenia**\n","\n","\n","---\n","\n","1. W sekcji [\"Synteza mowy -TTS\"](https://colab.research.google.com/drive/13jIPCHHTnZzWkjnuFPRM8fPZlH8Vw-5g#scrollTo=FplH5spO88e1) proszę uruchomić każdy z algorytmów z wymyśloną przez Państwa sekwencją słów i zapisać wynikowe pliki dźwiękowe z syntetyzowaną mową na dysk, a następnie:\n","  * na podstawie własnych wrażeń słuchowych opisać różnice między algorytmami ([Real-Time Voice Cloning](https://colab.research.google.com/drive/13jIPCHHTnZzWkjnuFPRM8fPZlH8Vw-5g#scrollTo=IhDehA7sT-Gx), [Google TTS (może działać po polsku)](https://colab.research.google.com/drive/13jIPCHHTnZzWkjnuFPRM8fPZlH8Vw-5g#scrollTo=Dp7nPgItKsIb), [Tacotron2 + Waveglow (angielski)](https://colab.research.google.com/drive/13jIPCHHTnZzWkjnuFPRM8fPZlH8Vw-5g#scrollTo=Yl46DA89KETr) i [Mozilla TTS (angielski)](https://colab.research.google.com/drive/13jIPCHHTnZzWkjnuFPRM8fPZlH8Vw-5g#scrollTo=B-e74RzUVqWU)), który z nich jest w stanie bardziej naturalnie syntetyzować ludzką mowę?\n","2. Proszę nagrać proste zdanie w języku angielskim (np. za pomocą [oprogramowania Audacity](https://manual.audacityteam.org/man/tutorial_your_first_recording.html)), a następnie:\n","  * dokonać syntezy tego samego zdania wpisując je do algorytmu [Google TTS](https://colab.research.google.com/drive/13jIPCHHTnZzWkjnuFPRM8fPZlH8Vw-5g#scrollTo=Dp7nPgItKsIb) (potrzebna będzie zmiana w kodzie z języka \"pl\" na \"en\"), [Tacotron2 + Waveglow](https://colab.research.google.com/drive/13jIPCHHTnZzWkjnuFPRM8fPZlH8Vw-5g#scrollTo=Yl46DA89KETr) oraz [Mozilla TTS](https://colab.research.google.com/drive/13jIPCHHTnZzWkjnuFPRM8fPZlH8Vw-5g#scrollTo=B-e74RzUVqWU) i zapisać wynikowe pliki dźwiękowe na dysk\n","  * dokonać analizy porównawczej spektrogramów (w [oprogramowaniu Audacity](https://manual.audacityteam.org/man/spectrogram_view.html)) i zapisać wnioski\n","3. W sekcji [\"Rozpoznawanie mowy - STT\"](https://colab.research.google.com/drive/13jIPCHHTnZzWkjnuFPRM8fPZlH8Vw-5g#scrollTo=HxWfharV9Mi-) proszę uruchomić każdy z algorytmów po kolei. Plik dźwiękowy może być plikiem wcześniej nagranym przez Was w Audacity (w języku angielskim) albo nagrany za pomocą notatnika (opcja upload/record w podsekcji \"Nagranie z mikrofonu lub plik zewnętrzny\"). <b>UWAGA:</b> tylko język angielski!\n","  * zapisać nagrany tekst w postaci pliku dźwiękowego, a w sprawozdaniu zapisać teksty, które udało sie rozpoznać każdemu z algorytmów (CMUSphinx, Google, Mozilla)\n","\n","**Sprawozdania mają formę dokumentu w dowolnej formie (.pdf, .docx, .odt, ...) oraz dołączonych do sprawozdania plików dźwiękowych z syntetyzowaną mową oraz nagraniem Waszego głosu.**"]},{"cell_type":"markdown","metadata":{"id":"KL1dl8vCTQ1V"},"source":["\n","\n","---\n","\n","\n","# Wprowadzenie do Google Colab\n","\n","\n","---\n","Krótkie wyjaśnienie jak korzystać z notatników w systemie Google Colab:\n","\n","https://colab.research.google.com/notebooks/basic_features_overview.ipynb\n"]},{"cell_type":"markdown","metadata":{"id":"371VkGAlrLV8"},"source":["# Ustawienia systemowe\n","Uzyskanie informacji o wykorzystywanym systemie \"w chmurze\", na którym będą wykonywane obliczenia."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lmt8rzUOlG7d"},"outputs":[],"source":["!cat /proc/cpuinfo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pc9kiWCMwEHe"},"outputs":[],"source":["!cat /proc/meminfo"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"5vnIrFJdwK0z"},"outputs":[{"name":"stdout","output_type":"stream","text":["zsh:1: command not found: nvidia-smi\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"BufkcFY1wNST"},"outputs":[],"source":["#@title Instalacja bibliotek\n","\n","%pip install numpy librosa matplotlib"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"njJosYCFweWL"},"outputs":[],"source":["#@title Zamontowanie przestrzeni dyskowej\n","#@markdown Będzie potrzebna autoryzacja do dysku Google.\n","import os \n","os.listdir('./')\n","['.config', 'sample_data', 'drive']\n","from google.colab import drive\n","drive.mount('/content/drive')\n","os.listdir('/content/drive/')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"i1aXHekew4TD"},"outputs":[],"source":["#@title Import zainstalowanych bibliotek\n","import soundfile as sf\n","import io\n","from six.moves.urllib.request import urlopen\n","\n","import librosa\n","import numpy as np\n","import librosa.display\n","from matplotlib import pyplot as plt\n","%matplotlib inline\n","!apt-get install -y -qq ffmpeg\n","\n","import IPython\n","import pandas as pd\n","import scipy\n","import sklearn\n","import os\n","\n","import seaborn as sns"]},{"cell_type":"markdown","metadata":{"id":"FplH5spO88e1"},"source":["# Synteza mowy - TTS"]},{"cell_type":"markdown","metadata":{"id":"IhDehA7sT-Gx"},"source":["## \"Real-Time Voice Cloning\"\n","\n","Synteza mowy na podstawie algorytmu opisanego w [pracy dyplomowej](https://matheo.uliege.be/bitstream/2268.2/6801/5/s123578Jemine2019.pdf) oraz [artykle](https://arxiv.org/pdf/1806.04558.pdf). Oryginalne repozytorium z kodem źródłowym znajduje się [tutaj](https://github.com/CorentinJ/Real-Time-Voice-Cloning). "]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"EfkTM9TjUCRx"},"outputs":[],"source":["#@title Krok 1.Inicjalizacja, instalacja oraz import niezbędnych bibliotek oraz modułów.\n","\n","#@markdown Uruchomienie za pomocą <code>CTRL+Enter</code> albo ikony <code>play</code>. \n","#@markdown Po dwukrotnym kliknięciu w okno tekstowe otworzy się panel edycji kodu źródłowego.\n","\n","%tensorflow_version 1.x\n","import os\n","from os.path import exists, join, basename, splitext\n","\n","# Skopiowanie repozytorium z kodami źródłowymi \n","git_repo_url = 'https://github.com/CorentinJ/Real-Time-Voice-Cloning.git'\n","project_name = splitext(basename(git_repo_url))[0]\n","#if not exists(project_name):\n","if(True):\n","  # klonowanie repozytorium\n","  !git clone -q --recursive {git_repo_url}\n","  # instalacja wymaganych bibliotek\n","  !cd {project_name} && pip install -q -r requirements.txt\n","  !pip install -q gdown\n","  !apt-get install -qq libportaudio2\n","  !pip install -q https://github.com/tugstugi/dl-colab-notebooks/archive/colab_utils.zip\n"," \n","  # Załadowanie wytrenowanych wcześniej modeli sieci\n","  #!cd {project_name} && wget https://github.com/blue-fish/Real-Time-Voice-Cloning/releases/download/v1.0/pretrained.zip && unzip -o pretrained.zip\n","  !wget -P {project_name}/encoder/saved_models/ https://www.dl.dropboxusercontent.com/s/uokycp4r9337fik/encoder.pt\n","  !wget -P {project_name}/synthesizer/saved_models/ https://www.dl.dropboxusercontent.com/s/wwan3xltliow1zv/synthesizer.pt\n","  !wget -P {project_name}/vocoder/saved_models/ https://www.dl.dropboxusercontent.com/s/4mfqmg2uy7trg8t/vocoder.pt\n","\n","\n","# Import niezbędnych bibliotek i modułów \n","import sys\n","sys.path.append(project_name)\n"," \n","from IPython.display import display, Audio, clear_output\n","from IPython.utils import io\n","import ipywidgets as widgets\n","import numpy as np\n","from dl_colab_notebooks.audio import record_audio, upload_audio\n","from scipy.io import wavfile\n"," \n","from synthesizer.inference import Synthesizer\n","from encoder import inference as encoder\n","from vocoder import inference as vocoder\n","from pathlib import Path\n","\n","# Załadowanie odpowiednich modeli sieci\n","encoder.load_model(project_name / Path(\"encoder/saved_models/encoder.pt\"))\n","synthesizer = Synthesizer(project_name / Path(\"synthesizer/saved_models/synthesizer.pt\"))\n","vocoder.load_model(project_name / Path(\"vocoder/saved_models/vocoder.pt\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"vBeMoBxLkDKN"},"outputs":[],"source":["#@title Krok 2. Nagranie z mikrofonu lub plik zewnętrzny\n","#@markdown <i>Możliwość nagrania bezpośrednio z mikrofonu lub załadowania pliku\n","#@markdown z dysku (*.mp3 lub *.wav)</i><br><br>\n","#@markdown <mark>W przypadku błędów podczas nagrywania z mikrofonu proszę\n","#@markdown o zmianę źródła nagrywania w pasku adresu przeglądarki (ikona kamerki)\n"," \n","SAMPLE_RATE = 44100\n","record_or_upload = \"Record\" #@param [\"Record\", \"Upload (.mp3 or .wav)\"]\n","record_seconds =   5#@param {type:\"number\", min:1, max:10, step:1}\n"," \n","embedding = None\n","def _compute_embedding(audio):\n","  display(Audio(audio, rate=SAMPLE_RATE, autoplay=True))\n","  wavfile.write('voice_recorded.wav', SAMPLE_RATE, (32767*audio).astype(np.int16))\n","  global embedding\n","  embedding = None\n","  embedding = encoder.embed_utterance(encoder.preprocess_wav(audio, SAMPLE_RATE))\n","  from google.colab import files\n","  files.download(\"voice_recorded.wav\")\n","def _record_audio(b):\n","  clear_output()\n","  audio = record_audio(record_seconds, sample_rate=SAMPLE_RATE)\n","  _compute_embedding(audio)\n","def _upload_audio(b):\n","  clear_output()\n","  audio = upload_audio(sample_rate=SAMPLE_RATE)\n","  _compute_embedding(audio)\n"," \n","if record_or_upload == \"Record\":\n","  button = widgets.Button(description=\"Record Your Voice\")\n","  button.on_click(_record_audio)\n","  display(button)\n","else:\n","  #button = widgets.Button(description=\"Upload Voice File\")\n","  #button.on_click(_upload_audio)\n","  _upload_audio(\"\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"WZjKkvGF1Y-i"},"outputs":[],"source":["#@title Krok 3. Synteza tekstu\n","text = \"Raz raz raz raz dwa dwa dwa \" #@param {type:\"string\"}\n","  \n","def synthesize(embed, text):\n","  print(\"Trwa synteza tekstu do audio...\")\n","  #with io.capture_output() as captured:\n","  specs = synthesizer.synthesize_spectrograms([text], [embed])\n","  generated_wav = vocoder.infer_waveform(specs[0])\n","  generated_wav = np.pad(generated_wav, (0, synthesizer.sample_rate), mode=\"constant\")\n","  wavfile.write('voice_synth_RTVC.wav', synthesizer.sample_rate, (32767*generated_wav).astype(np.int16))\n","  from google.colab import files\n","  files.download(\"voice_synth_RTVC.wav\")\n","  clear_output()\n","  display(Audio(generated_wav, rate=synthesizer.sample_rate, autoplay=True))\n"," \n","if embedding is None:\n","  print(\"first record a voice or upload a voice file!\")\n","else:\n","  synthesize(embedding, text)"]},{"cell_type":"markdown","metadata":{"id":"Dp7nPgItKsIb"},"source":["## Google TTS\n","\n","Synteza mowy na podstawie algorytmu [Google Translate Text-to-Speech](https://cloud.google.com/text-to-speech). Oryginalne repozytorium z kodem źródłowym i portowaniem do Python znajduje się [tutaj](https://gtts.readthedocs.io/en/latest/index.html). "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WymsRTE-7OpX"},"outputs":[],"source":["#@title Krok 1. Instalacja \n","!pip install gTTS\n","from scipy.io import wavfile"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"9cXLNvYv7LYz"},"outputs":[],"source":["#@title Krok2. Synteza tekstu\n","#@markdown Wpisz zdanie do syntezy i język syntezy\n","text = \"Niez\\u0142ego strzeli\\u0142e\\u015B gola. Pi\\u0119kny Lolo!\" #@param {type:\"string\"}\n","\n","from gtts import gTTS\n","from IPython.display import Audio\n","\n","lang = 'pl' #@param [\"en\", \"pl\"]\n","tts = gTTS(text,lang=lang)\n","sound_file = 'voice_synth_TTS.wav'\n","tts.save('voice_synth_TTS.wav')\n","\n","from google.colab import files\n","files.download(\"voice_synth_TTS.wav\")\n","\n","Audio(sound_file, autoplay=True)"]},{"cell_type":"markdown","metadata":{"id":"Yl46DA89KETr"},"source":["## Tacotron2 + Waveglow\n","\n","Synteza mowy na podstawie algorytmów [NVIDIA/tacotron2](https://github.com/NVIDIA/tacotron2) oraz [NVIDIA/waveglow](https://github.com/NVIDIA/waveglow). Kody źródłowe na podstawie [tego notatnika](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/NVidia_Tacotron2_Waveglow.ipynb).\n","\n","**WAŻNE.**\n","Do uruchomienia będzie potrzebne ponowne uruchomienie środowiska wykonawczego >> <code>CTRL + M + .</code> i zakończenie danej sesji >> <code>Menu -> Środowisko wykonawcze -> zarządzaj sesjami -> Zakończ</code>, a następnie uruchomienie kodu w tej sekcji od początku (Krok 1. poniżej)."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"r8O0caOt5lA2"},"outputs":[],"source":["#@title Krok 1. Instalacja\n","#!pip install tensorflow==1.15\n","%tensorflow_version 1.x\n","import os\n","from os.path import exists, join, basename, splitext\n","\n","git_repo_url = 'https://github.com/NVIDIA/tacotron2.git'\n","project_name = splitext(basename(git_repo_url))[0]\n","if not exists(project_name):\n","  # clone and install\n","  !git clone -q --recursive {git_repo_url}\n","  !cd {project_name}/waveglow && git checkout 9168aea\n","  !pip install -q librosa unidecode\n","  !pip install -q --upgrade gdown\n","  \n","import sys\n","sys.path.append(join(project_name, 'waveglow/'))\n","sys.path.append(project_name)\n","import time\n","import gdown\n","import matplotlib\n","import matplotlib.pylab as plt\n","plt.rcParams[\"axes.grid\"] = False"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"YCSZQKzE6Fq7"},"outputs":[],"source":["#@title Krok 2. Pobieranie wytrenowanych modeli sieci\n","import gdown\n","tacotron2_pretrained_model = 'tacotron2_statedict.pt'\n","if not exists(tacotron2_pretrained_model):\n","  # download the Tacotron2 pretrained model\n","  gdown.download(f\"https://drive.google.com/uc?export=download&confirm=pbef&id=1c5ZTuT7J08wLUoVZ2KkUs_VdZuJ86ZqA\",tacotron2_pretrained_model)\n","  #gdown.download('https://drive.google.com/open?id=1c5ZTuT7J08wLUoVZ2KkUs_VdZuJ86ZqA', tacotron2_pretrained_model, quiet=False)\n","waveglow_pretrained_model = 'waveglow_old.pt'\n","if not exists(waveglow_pretrained_model):\n","  # download the Waveglow pretrained model  \n","  gdown.download(f\"https://drive.google.com/uc?export=download&confirm=pbef&id=1WsibBTsuRg_SF2Z6L6NFRTT-NjEy1oTx\",waveglow_pretrained_model)\n","  #gdown.download('https://drive.google.com/open?id=1WsibBTsuRg_SF2Z6L6NFRTT-NjEy1oTx', waveglow_pretrained_model, quiet=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"25Jr6k0W6QA0"},"outputs":[],"source":["#@title Krok 3. Inicjalizacja sieci i import bibliotek\n","#@markdown Kod źródłowy zapożyczony z [tego źródła](https://github.com/NVIDIA/tacotron2/blob/master/inference.ipynb) i uaktualniony do działania w Colab.\n","import IPython.display as ipd\n","import numpy as np\n","import torch\n","\n","from hparams import create_hparams\n","from model import Tacotron2\n","from layers import TacotronSTFT\n","from audio_processing import griffin_lim\n","from text import text_to_sequence\n","from denoiser import Denoiser\n","\n","def plot_data(data, figsize=(16, 4)):\n","    fig, axes = plt.subplots(1, len(data), figsize=figsize)\n","    for i in range(len(data)):\n","        axes[i].imshow(data[i], aspect='auto', origin='bottom', \n","                       interpolation='none', cmap='viridis')\n","\n","torch.set_grad_enabled(False)\n","        \n","# initialize Tacotron2 with the pretrained model\n","hparams = create_hparams()\n","hparams.sampling_rate = 22050\n","model = Tacotron2(hparams)\n","model.load_state_dict(torch.load(tacotron2_pretrained_model)['state_dict'])\n","_ = model.cuda().eval()#.half()\n","\n","# initialize Waveglow with the pretrained model\n","# waveglow = torch.load(waveglow_pretrained_model)['model']\n","# WORKAROUND for: https://github.com/NVIDIA/tacotron2/issues/182\n","import json\n","from glow import WaveGlow\n","waveglow_config = json.load(open('%s/waveglow/config.json' % project_name))['waveglow_config']\n","waveglow = WaveGlow(**waveglow_config)\n","waveglow.load_state_dict(torch.load(waveglow_pretrained_model)['model'].state_dict())\n","_ = waveglow.cuda().eval()#.half()\n","for k in waveglow.convinv:\n","    k.float()\n","denoiser = Denoiser(waveglow)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"m02yezS_JbBI"},"outputs":[],"source":["#@title Krok 4. Synteza tekstu\n","\n","TEXT = \"Basics of Multimedia is awesome!\" #@param {type:\"string\"}\n","\n","sequence = np.array(text_to_sequence(TEXT, ['english_cleaners']))[None, :]\n","sequence = torch.autograd.Variable(torch.from_numpy(sequence)).long()\n","sequence = sequence.cuda()\n","\n","mel_outputs, mel_outputs_postnet, _, alignments = model.inference(sequence)\n","#plot_data((mel_outputs.data.cpu().numpy()[0],\n","#           mel_outputs_postnet.data.cpu().numpy()[0],\n","#           alignments.data.cpu().numpy()[0].T))\n","\n","audio = waveglow.infer(mel_outputs_postnet, sigma=0.666)\n","ipd.Audio(audio[0].data.cpu().numpy(), rate=hparams.sampling_rate)\n","\n","from scipy.io import wavfile\n","wavfile.write('voice_synth_T2WaveGlow.wav', hparams.sampling_rate, (32767*audio[0].data.cpu().numpy()).astype(np.int16))\n","from google.colab import files\n","files.download(\"voice_synth_T2WaveGlow.wav\")\n","\n","# remove waveglow bias\n","audio_denoised = denoiser(audio, strength=0.01)[:, 0]\n","ipd.Audio(audio_denoised.cpu().numpy(), rate=hparams.sampling_rate)\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"B-e74RzUVqWU"},"source":["## Mozilla TTS\n","\n","Synteza mowy na podstawie algorytmu opisanego w [mozilla/TTS](https://github.com/mozilla/TTS/) oraz dodatkowo z wykjorzystaniem sieci [erogol/WaveRNN](https://github.com/erogol/WaveRNN).\n","\n","**WAŻNE.**\n","Do uruchomienia może być potrzebne ponowne uruchomienie środowiska wykonawczego >> <code>CTRL + M + .</code>, a w niektórych przypadkach zakończenie danej sesji >> <code>Środowisko wykonawcze -> zarządzaj sesjami -> Zakończ</code>, a następnie uruchomienie kodu w tej sekcji od początku.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e-Hdw1q_X8JX"},"outputs":[],"source":["#@title Krok 1. Instalacja i import bibliotek\n","import os\n","import time\n","from os.path import exists, join, basename, splitext\n","\n","git_repo_url = 'https://github.com/mozilla/TTS.git'\n","project_name = splitext(basename(git_repo_url))[0]\n","if not exists(project_name):\n","  !git clone -q {git_repo_url}\n","  !cd {project_name} && git checkout Tacotron2-iter-260K-824c091\n","  !pip install -q --upgrade gdown\n","  !pip install -q lws librosa Unidecode==0.4.20 tensorboardX git+git://github.com/bootphon/phonemizer@master localimport\n","  !apt-get install -y espeak\n","git_repo_url = 'https://github.com/erogol/WaveRNN.git'\n","project_name = splitext(basename(git_repo_url))[0]\n","if not exists(project_name):\n","  !git clone -q {git_repo_url}\n","  !cd {project_name} && git checkout 8a1c152 && pip install -q -r requirements.txt\n","\n","  \n","import sys\n","sys.path.append('TTS')\n","sys.path.append('WaveRNN')\n","!pip install localimport\n","!pip install gdown\n","from localimport import localimport\n","import gdown\n","  \n","from IPython.display import Audio, display"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"klsVLR6w_u4P"},"outputs":[],"source":["#@title Krok 2. Pobieranie wytrenowanych modeli sieci\n","# WaveRNN\n","\n","!mkdir -p wavernn_models tts_models\n","wavernn_pretrained_model = 'wavernn_models/checkpoint_433000.pth.tar'\n","if not exists(wavernn_pretrained_model):\n","  gdown.download(f\"https://drive.google.com/uc?export=download&confirm=pbef&id=12GRFk5mcTDXqAdO5mR81E-DpTk8v2YS9\",wavernn_pretrained_model)\n","  #gdown.download('https://drive.google.com/uc?id=12GRFk5mcTDXqAdO5mR81E-DpTk8v2YS9', wavernn_pretrained_model, quiet=False)\n","wavernn_pretrained_model_config = 'wavernn_models/config.json'\n","if not exists(wavernn_pretrained_model_config):\n","  gdown.download(f\"https://drive.google.com/uc?export=download&confirm=pbef&id=1kiAGjq83wM3POG736GoyWOOcqwXhBulv\",wavernn_pretrained_model_config)\n","  #gdown.download('https://drive.google.com/uc?id=1kiAGjq83wM3POG736GoyWOOcqwXhBulv', wavernn_pretrained_model_config, quiet=False)\n","    \n","# TTS\n","tts_pretrained_model = 'tts_models/checkpoint_261000.pth.tar'\n","if not exists(tts_pretrained_model):\n","  gdown.download(f\"https://drive.google.com/uc?export=download&confirm=pbef&id=1otOqpixEsHf7SbOZIcttv3O7pG0EadDx\",tts_pretrained_model)\n","  #gdown.download(f'https://drive.google.com/uc?id=1otOqpixEsHf7SbOZIcttv3O7pG0EadDx', tts_pretrained_model, quiet=False)\n","tts_pretrained_model_config = 'tts_models/config.json'\n","if not exists(tts_pretrained_model_config):\n","  gdown.download(f\"https://drive.google.com/uc?export=download&confirm=pbef&id=1IJaGo0BdMQjbnCcOL4fPOieOEWMOsXE-\",tts_pretrained_model_config)\n","  #gdown.download('https://drive.google.com/uc?id=1IJaGo0BdMQjbnCcOL4fPOieOEWMOsXE-', tts_pretrained_model_config, quiet=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Ft1LHHdkA2Yc"},"outputs":[],"source":["#@title Krok 3. Inicjalizacja sieci\n","#@markdown Kod źródłowy zapożyczony [ze źródła](https://github.com/tugstugi/dl-colab-notebooks/blob/master/notebooks/Mozilla_TTS_WaveRNN.ipynb).\n","import io\n","import torch \n","import time\n","import numpy as np\n","from collections import OrderedDict\n","from matplotlib import pylab as plt\n","import IPython\n","\n","%pylab inline\n","rcParams[\"figure.figsize\"] = (16,5)\n","\n","import librosa\n","import librosa.display\n","\n","!pip install phonemizer\n","!pip install unidecode\n","\n","from TTS.models.tacotron import Tacotron \n","from TTS.layers import *\n","from TTS.utils.data import *\n","from TTS.utils.audio import AudioProcessor\n","from TTS.utils.generic_utils import load_config, setup_model\n","from TTS.utils.text import text_to_sequence\n","from TTS.utils.synthesis import synthesis\n","from TTS.utils.visual import visualize\n","\n","def tts(model, text, CONFIG, use_cuda, ap, use_gl, speaker_id=None, figures=True):\n","    t_1 = time.time()\n","    waveform, alignment, mel_spec, mel_postnet_spec, stop_tokens = synthesis(model, text, CONFIG, use_cuda, ap, truncated=True, enable_eos_bos_chars=CONFIG.enable_eos_bos_chars)\n","    if CONFIG.model == \"Tacotron\" and not use_gl:\n","        mel_postnet_spec = ap.out_linear_to_mel(mel_postnet_spec.T).T\n","    if not use_gl:\n","        waveform = wavernn.generate(torch.FloatTensor(mel_postnet_spec.T).unsqueeze(0).cuda(), batched=batched_wavernn, target=11000, overlap=550)\n","\n","    print(\" >  Run-time: {}\".format(time.time() - t_1))\n","    if figures:                                                                                                         \n","        visualize(alignment, mel_postnet_spec, stop_tokens, text, ap.hop_length, CONFIG, mel_spec)                                                                       \n","    IPython.display.display(Audio(waveform, rate=CONFIG.audio['sample_rate']))  \n","    #os.makedirs(OUT_FOLDER, exist_ok=True)\n","    #file_name = text.replace(\" \", \"_\").replace(\".\",\"\") + \".wav\"\n","    #out_path = os.path.join(OUT_FOLDER, file_name)\n","    #ap.save_wav(waveform, out_path)\n","    return alignment, mel_postnet_spec, stop_tokens, waveform\n","  \n","use_cuda = True\n","batched_wavernn = True\n","\n","# initialize TTS\n","CONFIG = load_config(tts_pretrained_model_config)\n","from TTS.utils.text.symbols import symbols, phonemes\n","# load the model\n","num_chars = len(phonemes) if CONFIG.use_phonemes else len(symbols)\n","model = setup_model(num_chars, CONFIG)\n","# load the audio processor\n","ap = AudioProcessor(**CONFIG.audio)         \n","# load model state\n","if use_cuda:\n","    cp = torch.load(tts_pretrained_model)\n","else:\n","    cp = torch.load(tts_pretrained_model, map_location=lambda storage, loc: storage)\n","\n","# load the model\n","model.load_state_dict(cp['model'])\n","if use_cuda:\n","    model.cuda()\n","model.eval()\n","print(cp['step'])\n","model.decoder.max_decoder_steps = 2000\n","\n","# initialize WaveRNN\n","VOCODER_CONFIG = load_config(wavernn_pretrained_model_config)\n","with localimport('/content/WaveRNN') as _importer:\n","  from models.wavernn import Model\n","bits = 10\n","\n","wavernn = Model(\n","        rnn_dims=512,\n","        fc_dims=512,\n","        mode=\"mold\",\n","        pad=2,\n","        upsample_factors=VOCODER_CONFIG.upsample_factors,  # set this depending on dataset\n","        feat_dims=VOCODER_CONFIG.audio[\"num_mels\"],\n","        compute_dims=128,\n","        res_out_dims=128,\n","        res_blocks=10,\n","        hop_length=ap.hop_length,\n","        sample_rate=ap.sample_rate,\n","    ).cuda()\n","check = torch.load(wavernn_pretrained_model)\n","wavernn.load_state_dict(check['model'])\n","if use_cuda:\n","    wavernn.cuda()\n","wavernn.eval()\n","print(check['step'])"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"GkxJ8J4dveaw"},"outputs":[],"source":["#@title Krok 4. Synteza tekstu\n","\n","SENTENCE = 'Oh my, my, my, the spring is coming and I feel Pola Mokotowskie very clear' #@param {type:\"string\"}\n","align, spec, stop_tokens, wav = tts(model, SENTENCE, CONFIG, use_cuda, ap, speaker_id=0, use_gl=False, figures=False)\n","\n","from scipy.io import wavfile\n","wavfile.write('voice_synth_MTTSWaveRNN.wav', 22050, (32767*wav).astype(np.int16))\n","from google.colab import files\n","files.download(\"voice_synth_MTTSWaveRNN.wav\")\n"]},{"cell_type":"markdown","metadata":{"id":"HxWfharV9Mi-"},"source":["#Rozpoznawanie mowy - STT"]},{"cell_type":"markdown","metadata":{"id":"uEPe8dq-k0Je"},"source":["## CMU Sphinx i Google STT\n","\n","### CMU Sphinx\n","\n","Biblioteka [CMUSphinx](https://cmusphinx.github.io/), która zawiera wiele alogorytmów i rozwiązań przeznaczonych do automatycznego rozpoznawania mowy. Biblioteka [PocketSphinx](https://github.com/cmusphinx/pocketsphinx-python), która implementuje wybrane algorytmy w jęzku Python.\n","\n","### Google Speech-To-Text\n","\n","Biblioteka [Google](https://cloud.google.com/speech-to-text).\n","\n","Uruchomienie obydwu ww. bibliotek jest możliwe tutaj dzięki integracji w narzędziu [SpeechRecognition](https://github.com/Uberi/speech_recognition).\n","\n","<br>\n","\n","**WAŻNE.**\n","Do uruchomienia może być potrzebne ponowne uruchomienie środowiska wykonawczego >> <code>CTRL + M + .</code>, a w niektórych przypadkach zakończenie danej sesji >> <code>Środowisko wykonawcze -> zarządzaj sesjami -> Zakończ</code>, a następnie uruchomienie kodu w tej sekcji od początku."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"5JkJUxkjk7SC"},"outputs":[],"source":["#@title Krok 1. Instalacja niezbędnych bibliotek i modułów\n","#@markdown Dla ciekawych - dwukrotne kliknięcie w nagłówek otworzy okno z kodem źródłowym.\n","\n","# Instalacja CMUSphinx\n","!apt-get install -y swig libpulse-dev\n","!swig -version\n","!pip3 install pocketsphinx\n","!pip3 list | grep pocketsphinx\n","\n","# Instalacja pakietu SpeechRecognition\n","!pip3 install SpeechRecognition\n","import os\n","\n","# Instalacja Mozilla DeepSpeech\n","!pip install deepspeech==0.6.0\n","if (os.path.isfile(\"deepspeech-0.6.0-models.tar.gz\")==False):\n","  !curl -LO https://github.com/mozilla/DeepSpeech/releases/download/v0.6.0/deepspeech-0.6.0-models.tar.gz\n","!tar -xvzf deepspeech-0.6.0-models.tar.gz\n","!ls -l ./deepspeech-0.6.0-models/\n","#!pip3 install --upgrade deepspeech\n","\n","# Instalacja\n","!pip install librosa\n","!apt-get install -y -qq ffmpeg\n","\n","# Inicjalizacja biblioteki\n","import pocketsphinx\n","\n"," \n","MODELDIR = os.path.join(os.path.dirname(pocketsphinx.__file__), 'model')\n"," \n","config = pocketsphinx.Decoder.default_config()\n","config.set_string('-hmm', os.path.join(MODELDIR, 'en-us'))\n","config.set_string('-lm', os.path.join(MODELDIR, 'en-us.lm.bin'))\n","config.set_string('-dict', os.path.join(MODELDIR, 'cmudict-en-us.dict'))\n"," \n","decoder = pocketsphinx.Decoder(config)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"BS9AuHWGk-pZ"},"outputs":[],"source":["#@title Krok 2. Nagranie z mikrofonu lub plik zewnętrzny\n","#@markdown <i>Możliwość nagrania bezpośrednio z mikrofonu lub załadowania pliku\n","#@markdown z dysku (*.mp3 lub *.wav)</i><br><br>\n","#@markdown <b>WAŻNE!</b> nagranie lub plik to zdanie w języku angielskim <br><br>\n","#@markdown Dla ciekawych - dwukrotne kliknięcie w nagłówek otworzy okno z kodem źródłowym.\n","\n","# Import i instalacja niezbędnych bibliotek i modułów do nagrywania i odtwarzania\n","!apt-get install -qq libportaudio2\n","!pip install -q https://github.com/tugstugi/dl-colab-notebooks/archive/colab_utils.zip\n","\n","import sys\n"," \n","from IPython.display import display, Audio, clear_output\n","from IPython.utils import io\n","import ipywidgets as widgets\n","import numpy as np\n","from dl_colab_notebooks.audio import record_audio, upload_audio\n","from scipy.io import wavfile\n"," \n","SAMPLE_RATE = 44100\n","record_or_upload = \"Record\" #@param [\"Record\", \"Upload (.mp3 or .wav)\"]\n","record_seconds =   10#@param {type:\"number\", min:1, max:10, step:1}\n"," \n","embedding = None\n","def _compute_embedding(audio):\n","  display(Audio(audio, rate=SAMPLE_RATE, autoplay=True))\n","  wavfile.write('voice_recorded_uploaded.wav', SAMPLE_RATE, (32767*audio).astype(np.int16))\n","  global embedding\n","  from google.colab import files\n","  files.download(\"voice_recorded_uploaded.wav\")\n","def _record_audio(b):\n","  clear_output()\n","  audio = record_audio(record_seconds, sample_rate=SAMPLE_RATE)\n","  _compute_embedding(audio)\n","def _upload_audio(b):\n","  clear_output()\n","  audio = upload_audio(sample_rate=SAMPLE_RATE)\n","  _compute_embedding(audio)\n"," \n","if record_or_upload == \"Record\":\n","  button = widgets.Button(description=\"Record Your Voice\")\n","  button.on_click(_record_audio)\n","  display(button)\n","else:\n","  #button = widgets.Button(description=\"Upload Voice File\")\n","  #button.on_click(_upload_audio)\n","  _upload_audio(\"\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"h7e3iwlqlCQo"},"outputs":[],"source":["#@title Krok 3. SpeechRecognition\n","#@markdown <i>Rozpoznawanie za pomocą algorytmu CMUSphinx i Google wcześniej nagranego lub\n","#@markdown wgranego pliku</i><br><br>\n","#@markdown Dla ciekawych - dwukrotne kliknięcie w nagłówek otworzy okno z kodem źródłowym.\n","\n","import speech_recognition as sr\n","from enum import Enum, unique\n"," \n","@unique\n","class ASREngine(Enum):\n","    sphinx = 0\n","    google = 1\n"," \n","def speech_to_text(filename: str, engine: ASREngine, language: str, show_all: bool = False) -> str:\n","    r = sr.Recognizer()\n","\n","    try:\n","      with sr.AudioFile(filename) as source:\n","          audio = r.record(source)\n"," \n","      asr_functions = {\n","          ASREngine.sphinx: r.recognize_sphinx,\n","          ASREngine.google: r.recognize_google,\n","      }\n"," \n","      response = asr_functions[engine](audio, language=language, show_all=show_all)\n","      return response\n","    except:\n","      print('Brak pliku audio\\nproszę wcześniej nagrać lub załadować plik')\n"," \n","filename = 'voice_recorded_uploaded.wav'\n","lang = 'en-US'\n"," \n","for asr_engine in ASREngine:\n","  try:\n","    response = speech_to_text(filename, asr_engine, language=lang)\n","    print('{0}: \"{1}\"'.format(asr_engine.name, response))\n","  except sr.UnknownValueError:\n","    print('{0} could not understand audio'.format(asr_engine.name))\n","  except sr.RequestError as e:\n","    print('{0} error: {0}'.format(asr_engine.name, e))"]},{"cell_type":"markdown","metadata":{"id":"K36NkIdDlIJY"},"source":["## Mozilla DeepSpeech\n","\n","Algorytm [Mozilla DeepSpeech](https://arxiv.org/abs/1412.5567) z kodem źródłowym [w tym miejscu](https://github.com/mozilla/DeepSpeech)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ssRBdaeNlO2v"},"outputs":[],"source":["#@title Krok 1. Uruchomienie DeepSpeech\n","#@markdown <i>Rozpoznawanie za pomocą algorytmu Mozilla DeepSpeech wcześniej nagranego lub\n","#@markdown wgranego pliku</i><br><br>\n","#@markdown Dla ciekawych - dwukrotne kliknięcie w nagłówek otworzy okno z kodem źródłowym.\n","import numpy as np\n","import deepspeech\n","\n","model_file_path = 'deepspeech-0.6.0-models/output_graph.pbmm'\n","beam_width = 500\n","model = deepspeech.Model(model_file_path, beam_width)\n","\n","# Add language model for better accuracy\n","lm_file_path = 'deepspeech-0.6.0-models/lm.binary'\n","trie_file_path = 'deepspeech-0.6.0-models/trie'\n","lm_alpha = 0.75\n","lm_beta = 1.85\n","model.enableDecoderWithLM(lm_file_path, trie_file_path, lm_alpha, lm_beta)\n","\n","import numpy as np\n","import wave \n","\n","def deepspeech_batch_stt(filename: str) -> str:\n","    import librosa    \n","    import soundfile as sf\n","\n","    # Get example audio file\n","    data, samplerate = sf.read(filename, dtype='float32')\n","    data = data.T\n","    data16kfloat = librosa.resample(data, samplerate, 16000)\n","    sf.write('voice16.wav', data16kfloat, 16000, subtype='PCM_16')\n","    data16, samplerate = sf.read('voice16.wav', dtype='int16')\n","    #return model.stt(data16)\n","    return model.stt(data16)\n","\n","filename = 'voice_recorded_uploaded.wav'\n","print('WYNIK: \"{}\"'.format(deepspeech_batch_stt(filename)))"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyP2peqWmY61uJacu2pvs3th","collapsed_sections":["371VkGAlrLV8","FplH5spO88e1","IhDehA7sT-Gx","Dp7nPgItKsIb","Yl46DA89KETr","B-e74RzUVqWU","HxWfharV9Mi-","uEPe8dq-k0Je","K36NkIdDlIJY"],"name":"Lab3_WMM.ipynb","provenance":[{"file_id":"13jIPCHHTnZzWkjnuFPRM8fPZlH8Vw-5g","timestamp":1653300828505}]},"interpreter":{"hash":"8a94588eda9d64d9e9a351ab8144e55b1fabf5113b54e67dd26a8c27df0381b3"},"kernelspec":{"display_name":"Python 3.10.4 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":0}
